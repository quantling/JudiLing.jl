<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Deep learning · JudiLing.jl</title><link rel="canonical" href="https://quantling.github.io/JudiLing.jl/man/deep_learning/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">JudiLing.jl</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Manual</span><ul><li><a class="tocitem" href="../input/">Loading data</a></li><li><a class="tocitem" href="../make_cue_matrix/">Make Cue Matrix</a></li><li><a class="tocitem" href="../make_semantic_matrix/">Make Semantic Matrix</a></li><li><a class="tocitem" href="../cholesky/">Cholesky</a></li><li><a class="tocitem" href="../make_adjacency_matrix/">Make Adjacency Matrix</a></li><li><a class="tocitem" href="../make_yt_matrix/">Make Yt Matrix</a></li><li><a class="tocitem" href="../find_path/">Find Paths</a></li><li><a class="tocitem" href="../eval/">Evaluation</a></li><li><a class="tocitem" href="../output/">Output</a></li><li><a class="tocitem" href="../test_combo/">Test Combo</a></li><li><a class="tocitem" href="../display/">Display</a></li><li><a class="tocitem" href="../utils/">Utils</a></li><li><a class="tocitem" href="../pickle/">Pickle</a></li><li><a class="tocitem" href="../pyndl/">Pyndl</a></li><li><a class="tocitem" href="../wh/">Widrow-Hoff Learning</a></li><li class="is-active"><a class="tocitem" href>Deep learning</a></li><li><a class="tocitem" href="../measures_func/">Measures function</a></li></ul></li><li><a class="tocitem" href="../all_manual/">All Manual index</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Manual</a></li><li class="is-active"><a href>Deep learning</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Deep learning</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/quantling/JudiLing.jl/blob/master/docs/src/man/deep_learning.md#L" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Deep-learning-in-JudiLing"><a class="docs-heading-anchor" href="#Deep-learning-in-JudiLing">Deep learning in JudiLing</a><a id="Deep-learning-in-JudiLing-1"></a><a class="docs-heading-anchor-permalink" href="#Deep-learning-in-JudiLing" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="JudiLing.predict_from_deep_model-Tuple{Chain, Union{SparseArrays.SparseMatrixCSC, Matrix}}" href="#JudiLing.predict_from_deep_model-Tuple{Chain, Union{SparseArrays.SparseMatrixCSC, Matrix}}"><code>JudiLing.predict_from_deep_model</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">predict_from_deep_model(model::Chain,
                        X::Union{SparseMatrixCSC,Matrix})</code></pre><p>Generates output of a model given input <code>X</code>.</p><p><strong>Obligatory arguments</strong></p><ul><li><code>model::Chain</code>: Model of type Flux.Chain, as generated by <code>get_and_train_model</code></li><li><code>X::Union{SparseMatrixCSC,Matrix}</code>: Input matrix of size (number<em>of</em>samples, inp<em>dim) where inp</em>dim is the input dimension of <code>model</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/quantling/JudiLing.jl/blob/8b0388fd423d349063005de404c39dc679ebf778/src/deep_learning.jl#LL637-L647">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="JudiLing.predict_shat-Tuple{Chain, Vector{Int64}}" href="#JudiLing.predict_shat-Tuple{Chain, Vector{Int64}}"><code>JudiLing.predict_shat</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">predict_shat(model::Chain,
             ci::Vector{Int})</code></pre><p>Predicts semantic vector shat given a deep learning comprehension model <code>model</code> and a list of indices of ngrams <code>ci</code>.</p><p><strong>Obligatory arguments</strong></p><ul><li><code>model::Chain</code>: Deep learning comprehension model as generated by <code>get_and_train_model</code></li><li><code>ci::Vector{Int}</code>: Vector of indices of ngrams in c vector. Essentially, this is a vector indicating which ngrams in a c vector are absent and which are present.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/quantling/JudiLing.jl/blob/8b0388fd423d349063005de404c39dc679ebf778/src/deep_learning.jl#LL656-L666">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="JudiLing.get_and_train_model-Tuple{Union{SparseArrays.SparseMatrixCSC, Matrix}, Union{SparseArrays.SparseMatrixCSC, Matrix}, Union{Missing, SparseArrays.SparseMatrixCSC, Matrix}, Union{Missing, SparseArrays.SparseMatrixCSC, Matrix}, Union{Missing, DataFrames.DataFrame}, Union{Missing, DataFrames.DataFrame}, Union{Missing, String, Symbol}, String}" href="#JudiLing.get_and_train_model-Tuple{Union{SparseArrays.SparseMatrixCSC, Matrix}, Union{SparseArrays.SparseMatrixCSC, Matrix}, Union{Missing, SparseArrays.SparseMatrixCSC, Matrix}, Union{Missing, SparseArrays.SparseMatrixCSC, Matrix}, Union{Missing, DataFrames.DataFrame}, Union{Missing, DataFrames.DataFrame}, Union{Missing, String, Symbol}, String}"><code>JudiLing.get_and_train_model</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">get_and_train_model(X_train::Union{SparseMatrixCSC,Matrix},
                    Y_train::Union{SparseMatrixCSC,Matrix},
                    X_val::Union{SparseMatrixCSC,Matrix,Missing},
                    Y_val::Union{SparseMatrixCSC,Matrix,Missing},
                    data_train::Union{DataFrame,Missing},
                    data_val::Union{DataFrame,Missing},
                    target_col::Union{Symbol,String,Missing},
                    model_outpath::String;
                    hidden_dim::Int=1000,
                    n_epochs::Int=100,
                    batchsize::Int=64,
                    loss_func::Function=Flux.mse,
                    optimizer=Flux.Adam(0.001)
                    model::Union{Missing, Chain}=missing,
                    early_stopping::Union{Missing, Int}=missing,
                    optimise_for_acc::Bool=false
                    return_losses::Bool=false,
                    verbose::Bool=true,
                    measures_func::Union{Missing, Function}=missing,
                    return_train_acc::Bool=false,
                    ...kargs
                    )</code></pre><p>Trains a deep learning model from <code>X_train</code> to <code>Y_train</code>, saving the model with either the highest validation accuracy or lowest validation loss (depending on <code>optimise_for_acc</code>) to <code>outpath</code>.</p><p>The default model looks like this:</p><pre><code class="language-none">inp_dim = size(X_train, 2)
out_dim = size(Y_train, 2)
Chain(Dense(inp_dim =&gt; hidden_dim, relu), Dense(hidden_dim =&gt; out_dim))</code></pre><p>Any other model with the same input and output dimensions can be provided to the function with the <code>model</code> argument. The default loss function is mean squared error, but any other loss function can be provded, as long as it fits with the model architecture.</p><p>By default the adam optimizer (Kingma and Ba, 2015) with learning rate 0.001 is used. You can provide any other optimizer. If you want to use a different learning rate, e.g. 0.01, provide <code>optimizer=Flux.Adam(0.01)</code>. If you do not want to use an optimizer at all, and simply use normal gradient descent, provide <code>optimizer=Descent(0.001)</code>, again replacing the learning rate with the learning rate of your preference.</p><p>Returns a named tuple with the following values:</p><ul><li><code>model</code>: the trained model</li><li><code>data_train</code>: the training data, including any measures if computed by <code>measures_func</code></li><li><code>data_val</code>: the validation data, including any measures if computed by <code>measures_func</code></li><li><code>losses_train</code>: The losses of the training data for each epoch.</li><li><code>losses_val</code>: The losses of the validation data after each epoch.</li><li><code>accs_train</code>: The accuracies of the training data after each epoch, if <code>return_train_acc=true</code>.</li><li><code>accs_val</code>: The accuracies of the validation data after each epoch.</li></ul><p><strong>Obligatory arguments</strong></p><ul><li><code>X_train::Union{SparseMatrixCSC,Matrix}</code>: training input matrix of dimension m x n</li><li><code>Y_train::Union{SparseMatrixCSC,Matrix}</code>: training output/target matrix of dimension m x k</li><li><code>X_train::Union{SparseMatrixCSC,Matrix}</code>: validation input matrix of dimension l x n</li><li><code>Y_train::Union{SparseMatrixCSC,Matrix}</code>: validation output/target matrix of dimension l x k</li><li><code>data_train::DataFrame</code>: training data</li><li><code>data_val::DataFrame</code>: validation data</li><li><code>target_col::Union{Symbol, String}</code>: column with target wordforms in data<em>train and data</em>val</li><li><code>model_outpath::String</code>: filepath to where final model should be stored (in .bson format)</li></ul><p><strong>Optional arguments</strong></p><ul><li><code>hidden_dim::Int=1000</code>: hidden dimension of the model</li><li><code>n_epochs::Int=100</code>: number of epochs for which the model should be trained</li><li><code>batchsize::Int=64</code>: batchsize during training</li><li><code>loss_func::Function=Flux.mse</code>: Loss function. Per default this is the mse loss, but other options might be a crossentropy loss (<code>Flux.crossentropy</code>). Make sure the model makes sense with the loss function!</li><li><code>optimizer=Flux.Adam(0.001)</code>: optimizer to use for training</li><li><code>model::Union{Missing, Chain} = missing</code>: A custom model can be provided for training. Its requirements are that it has to correspond to the input and output size of the training and validation data</li><li><code>early_stopping::Union{Missing, Int}=missing</code>: If <code>missing</code>, no early stopping is used. Otherwise <code>early_stopping</code> indicates how many epochs have to pass without improvement in validation accuracy before the training is stopped.</li><li><code>optimise_for_acc::Bool=false</code>: if true, keep model with highest validation <em>accuracy</em>. If false, keep model with lowest validation <em>loss</em>.</li><li><code>return_losses::Bool=false</code>: whether additional to the model per-epoch losses for the training and test data as well as per-epoch accuracy on the validation data should be returned</li><li><code>verbose::Bool=true</code>: Turn on verbose mode</li><li><code>measures_func::Union{Missing, Function}=missing</code>: A measures function which is run at the end of every epoch. For more information see <a href="../measures_func/#The-measures_func-argument">The <code>measures_func</code> argument</a>. If a measure is tagged for each epoch, the one tagged with &quot;final&quot; will be the one for the finally returned model.</li><li><code>return_train_acc::Bool=false</code>: If true, a vector with training accuracies is returned at the end of the training.</li><li><code>...kargs</code>: any additional keyword arguments are passed to the measures_func</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/quantling/JudiLing.jl/blob/8b0388fd423d349063005de404c39dc679ebf778/src/deep_learning.jl#LL18-L89">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="JudiLing.get_and_train_model-Tuple{Union{SparseArrays.SparseMatrixCSC, Matrix}, Union{SparseArrays.SparseMatrixCSC, Matrix}, String}" href="#JudiLing.get_and_train_model-Tuple{Union{SparseArrays.SparseMatrixCSC, Matrix}, Union{SparseArrays.SparseMatrixCSC, Matrix}, String}"><code>JudiLing.get_and_train_model</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">get_and_train_model(X_train::Union{SparseMatrixCSC,Matrix},
                    Y_train::Union{SparseMatrixCSC,Matrix},
                    model_outpath::String;
                    data_train::Union{Missing, DataFrame}=missing,
                    target_col::Union{Missing, Symbol, String}=missing,
                    hidden_dim::Int=1000,
                    n_epochs::Int=100,
                    batchsize::Int=64,
                    loss_func::Function=Flux.mse,
                    optimizer=Flux.Adam(0.001),
                    model::Union{Missing, Chain} = missing,
                    return_losses::Bool=false,
                    verbose::Bool=true,
                    measures_func::Union{Missing, Function}=missing,
                    return_train_acc::Bool=false,
                    ...kargs)</code></pre><p>Trains a deep learning model from <code>X_train</code> to <code>Y_train</code>, saving the model after n_epochs epochs. The default model looks like this:</p><pre><code class="language-none">inp_dim = size(X_train, 2)
out_dim = size(Y_train, 2)
Chain(Dense(inp_dim =&gt; hidden_dim, relu), Dense(hidden_dim =&gt; out_dim))</code></pre><p>Any other model with the same input and output dimensions can be provided to the function with the <code>model</code> argument. The default loss function is mean squared error, but any other loss function can be provded, as long as it fits with the model architecture.</p><p>By default the adam optimizer (Kingma and Ba, 2015) with learning rate 0.001 is used. You can provide any other optimizer. If you want to use a different learning rate, e.g. 0.01, provide <code>optimizer=Flux.Adam(0.01)</code>. If you do not want to use an optimizer at all, and simply use normal gradient descent, provide <code>optimizer=Descent(0.001)</code>, again replacing the learning rate with the learning rate of your preference.</p><p>Returns a named tuple with the following values:</p><ul><li><code>model</code>: the trained model</li><li><code>data_train</code>: the data, including any measures if computed by <code>measures_func</code></li><li><code>data_val</code>: missing for this function</li><li><code>losses_train</code>: The losses of the training data for each epoch.</li><li><code>losses_val</code>: missing for this function</li><li><code>accs_train</code>: The accuracies of the training data after each epoch, if <code>return_train_acc=true</code>.</li><li><code>accs_val</code>: missing for this function</li></ul><p><strong>Obligatory arguments</strong></p><ul><li><code>X_train::Union{SparseMatrixCSC,Matrix}</code>: training input matrix of dimension m x n</li><li><code>Y_train::Union{SparseMatrixCSC,Matrix}</code>: training output/target matrix of dimension m x k</li><li><code>model_outpath::String</code>: filepath to where final model should be stored (in .bson format)</li></ul><p><strong>Optional arguments</strong></p><ul><li><code>data_train::Union{Missing, DataFrame}=missing</code>: The training data. Only necessary if a measures<em>func is included or return</em>train_acc=true.</li><li><code>target_col::Union{Missing, Symbol, String}=missing</code>: The column with target word forms in the training data. Only necessary if a measures<em>func is included or return</em>train_acc=true.</li><li><code>hidden_dim::Int=1000</code>: hidden dimension of the model</li><li><code>n_epochs::Int=100</code>: number of epochs for which the model should be trained</li><li><code>batchsize::Int=64</code>: batchsize during training</li><li><code>loss_func::Function=Flux.mse</code>: Loss function. Per default this is the mse loss, but other options might be a crossentropy loss (<code>Flux.crossentropy</code>). Make sure the model makes sense with the loss function!</li><li><code>optimizer=Flux.Adam(0.001)</code>: optimizer to use for training</li><li><code>model::Union{Missing, Chain} = missing</code>: A custom model can be provided for training. Its requirements are that it has to correspond to the input and output size of the training and validation data</li><li><code>return_losses::Bool=false</code>: whether additional to the model per-epoch losses for the training and test data as well as per-epoch accuracy on the validation data should be returned</li><li><code>verbose::Bool=true</code>: Turn on verbose mode</li><li><code>measures_func::Union{Missing, Function}=missing</code>: A measures function which is run at the end of every epoch. For more information see <a href="../measures_func/#The-measures_func-argument">The <code>measures_func</code> argument</a>.</li><li><code>return_train_acc::Bool=false</code>: If true, a vector with training accuracies is returned at the end of the training.</li><li><code>...kargs</code>: any additional keyword arguments are passed to the measures_func</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/quantling/JudiLing.jl/blob/8b0388fd423d349063005de404c39dc679ebf778/src/deep_learning.jl#LL320-L380">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="JudiLing.fiddl-Tuple{Union{SparseArrays.SparseMatrixCSC, Matrix}, Union{SparseArrays.SparseMatrixCSC, Matrix}, Vector, DataFrames.DataFrame, Union{String, Symbol}, String}" href="#JudiLing.fiddl-Tuple{Union{SparseArrays.SparseMatrixCSC, Matrix}, Union{SparseArrays.SparseMatrixCSC, Matrix}, Vector, DataFrames.DataFrame, Union{String, Symbol}, String}"><code>JudiLing.fiddl</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">fiddl(X_train::Union{SparseMatrixCSC,Matrix},
        Y_train::Union{SparseMatrixCSC,Matrix},
        learn_seq::Vector,
        data::DataFrame,
        target_col::Union{Symbol, String},
        model_outpath::String;
        hidden_dim::Int=1000,
        batchsize::Int=64,
        loss_func::Function=Flux.mse,
        optimizer=Flux.Adam(0.001),
        model::Union{Missing, Chain} = missing,
        return_losses::Bool=false,
        verbose::Bool=true,
        n_batch_eval::Int=100,
        compute_accuracy::Bool=true,
        measures_func::Union{Function, Missing}=missing,
        kargs...)</code></pre><p>Trains a deep learning model using the FIDDL method (frequency-informed deep discriminative learning). Optionally, after each <code>n_batch_eval</code> batches <code>measures_func</code> can be run to compute any measures which are then added to the data.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>If you get an OutOfMemory error, chances are that this is due to the <code>eval_SC</code> function being evaluated after each <code>n_batch_eval</code> batches. Setting <code>compute_accuracy=false</code> disables computing the mapping accuracy.</p></div></div><p>Returns a named tuple with the following values:</p><ul><li><code>model</code>: the trained model</li><li><code>data</code>: the data, including any measures if computed by <code>measures_func</code></li><li><code>losses_train</code>: The losses of the data the model is trained on within each <code>n_batch_eval</code> batches.</li><li><code>losses</code>: The losses of the full dataset after each <code>n_batch_eval</code> batches.</li><li><code>accs</code>: The accuracies of the full dataset after each <code>n_batch_eval</code> batches.</li></ul><p><strong>Obligatory arguments</strong></p><ul><li><code>X_train::Union{SparseMatrixCSC,Matrix}</code>: training input matrix of dimension m x n</li><li><code>Y_train::Union{SparseMatrixCSC,Matrix}</code>: training output/target matrix of dimension m x k</li><li><code>learn_seq::Vector</code>: List of indices in the order that the vectors in X<em>train and Y</em>train should be presented to the model for training.</li><li><code>data::DataFrame</code>: The full data.</li><li><code>target_col::Union{Symbol, String}</code>: The column with target word forms in the data.</li><li><code>model_outpath::String</code>: filepath to where final model should be stored (in .bson format)</li></ul><p><strong>Optional arguments</strong></p><ul><li><code>hidden_dim::Int=1000</code>: hidden dimension of the model</li><li><code>n_epochs::Int=100</code>: number of epochs for which the model should be trained</li><li><code>batchsize::Int=64</code>: batchsize during training</li><li><code>loss_func::Function=Flux.mse</code>: Loss function. Per default this is the mse loss, but other options might be a crossentropy loss (<code>Flux.crossentropy</code>). Make sure the model makes sense with the loss function!</li><li><code>optimizer=Flux.Adam(0.001)</code>: optimizer to use for training</li><li><code>model::Union{Missing, Chain} = missing</code>: A custom model can be provided for training. Its requirements are that it has to correspond to the input and output size of the training and validation data</li><li><code>return_losses::Bool=false</code>: whether additional to the model per-epoch losses for the training and test data as well as per-epoch accuracy on the validation data should be returned</li><li><code>verbose::Bool=true</code>: Turn on verbose mode</li><li><code>n_batch_eval::Int=100</code>: Loss, accuracy and <code>measures_func</code> are evaluated every <code>n_batch_eval</code> batches.</li><li><code>compute_accuracy::Bool=true</code>: Whether accuracy should be computed every <code>n_batch_eval</code> batches.</li><li><code>measures_func::Union{Missing, Function}=missing</code>: A measures function which is run each <code>n_batch_eval</code> batches. For more information see <a href="../measures_func/#The-measures_func-argument">The <code>measures_func</code> argument</a>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/quantling/JudiLing.jl/blob/8b0388fd423d349063005de404c39dc679ebf778/src/deep_learning.jl#LL421-L473">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../wh/">« Widrow-Hoff Learning</a><a class="docs-footer-nextpage" href="../measures_func/">Measures function »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Friday 23 August 2024 13:01">Friday 23 August 2024</span>. Using Julia version 1.10.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
